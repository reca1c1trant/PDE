# PDE Causal AR Training Configuration

# Dataset
dataset:
  path: "/home/msai/song0304/code/PDE/data" 
  temporal_length: 16
  train_ratio: 0.9
  seed: 42

# DataLoader
dataloader:
  batch_size: 2  # Per GPU
  num_workers: 4
  pin_memory: true

# Model
model:
  in_channels: 6
  encoder_hidden_dim: 4096

  # Acceleration
  # NOTE: FlashAttention-2 does not support custom 4D attention masks (block causal)
  # Set to false to use eager attention which supports custom masks
  use_flash_attention: false
  gradient_checkpointing: true  # Trade speed for memory

  # Llama Transformer (8B config)
  transformer:
    hidden_size: 4096
    num_hidden_layers: 24
    num_attention_heads: 32
    num_key_value_heads: 8  # GQA
    intermediate_size: 14336
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

# Training
training:
  max_steps: 100000  # Total training steps
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  grad_clip: 1.0
  min_lr: 1.0e-6
  use_scheduler: true
  warmup_steps: 1000

  # Distributed Training
  use_fsdp: true  # Use FSDP instead of DDP for large models
  fsdp_cpu_offload: false  # Offload optimizer states to CPU (saves memory, slower)

  # Acceleration
  mixed_precision: "bf16"  # bf16 / fp16 / no
  gradient_accumulation_steps: 1

  # Checkpointing
  save_every_steps: 1000
  eval_every_steps: 500
  keep_last_n_checkpoints: 3

# Logging
logging:
  project: "pde-solver"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints"
  log_interval: 10
