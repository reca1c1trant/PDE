# Burgers2D LoRA Finetuning - Resume Configuration
# Resume from previous checkpoint and continue training

# Resume settings
resume:
  checkpoint_path: "./checkpoints_burgers_lora_v2/best_lora.pt"
  reset_scheduler: true  # Reset LR scheduler with new warmup

# Model (must match original training)
model:
  pretrained_path: "./checkpoints_e2e_medium/best.pt"

  in_channels: 6
  noise_level: 0.0
  use_flash_attention: false
  gradient_checkpointing: true

  encoder:
    version: "v2"
    channels: [64, 128, 256]
    use_resblock: true

  transformer:
    hidden_size: 768
    num_hidden_layers: 10
    num_attention_heads: 12
    num_key_value_heads: 4
    intermediate_size: 3072
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

  lora:
    r: 16
    alpha: 32
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

# Dataset (same as original)
dataset:
  path: "./burgers2d_nu0.1_0.15_res128_t1000_n100.h5"
  temporal_length: 16
  train_ratio: 0.9
  seed: 42
  clips_per_sample: 976

# Physics parameters
physics:
  dt: 0.001001001
  dx: 0.0078125

# DataLoader
dataloader:
  batch_size: 8
  num_workers: 4
  pin_memory: true

# Training - extended epochs with new LR
training:
  max_epochs: 40  # Extended from 20 to 40
  warmup_steps: 100  # New warmup from 1e-5
  learning_rate: 1.0e-5  # Start from 1e-5 (previous min was 1e-6)
  min_lr: 1.0e-6
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  grad_clip: 1.0

  lambda_data: 0.0
  lambda_pde: 1.0

  mixed_precision: "bf16"
  gradient_accumulation_steps: 1
  eval_interval: 100

# Logging - use same save_dir to continue saving to same location
logging:
  project: "pde-burgers-lora"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints_burgers_lora_v2"
  log_interval: 10
