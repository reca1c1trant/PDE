# SUT-FNO V2 Training Configuration - Burgers Equation
#
# Spectral U-Net Transformer + FNO V2
# Stage 1: Pretrain with PDE Loss + RMSE Loss + Gradient Loss
#
# V2 Features:
# - Learnable Spectral Attention (adaptive frequency weighting)
# - Gradient-Aware Loss (spatial derivative supervision)
# - Physics-informed high-frequency emphasis
#
# Training Budget:
#   - 90 train samples × 100 clips/sample = 9000 clips/epoch
#   - batch_size=6 → ~1500 steps/epoch
#   - 50 epochs → ~75000 total steps

# Model Configuration
model:
  in_channels: 6
  gradient_checkpointing: true

  # Spectral Encoder V2
  encoder:
    channels: [64, 128, 256]           # Output channels at each scale
    fno_modes: [32, 16, 8]             # FNO modes at 64x64, 32x32, 16x16
    spectral_skip_modes: [32, 16, 8]   # Modes to keep for skip connections
    use_spectral_attention: true       # V2: Enable Learnable Spectral Attention

  # Spectral Decoder
  decoder:
    channels: [256, 128, 64]           # Output channels at each scale
    fno_modes: [16, 24, 32]            # FNO modes at 32x32, 64x64, 128x128

  # Llama Transformer (same as original)
  transformer:
    hidden_size: 768
    num_hidden_layers: 10
    num_attention_heads: 12
    num_key_value_heads: 4
    intermediate_size: 3072
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

  # FNO Refinement Head
  fno_head:
    modes: 32                          # Full resolution, use more modes
    width: 64                          # Hidden width
    n_layers: 3                        # Number of Fourier layers

# Dataset
dataset:
  path: "./burgers2d_nu0.1_0.15_res128_t1000_n100.h5"
  temporal_length: 16                  # Will load 17 frames (16+1)
  train_ratio: 0.9
  seed: 42
  clips_per_sample: 100                # K clips per sample per epoch

# Physics parameters
physics:
  dt: 0.001001001                      # 1/999, time step
  Lx: 1.0                              # Domain size x
  Ly: 1.0                              # Domain size y

# DataLoader
dataloader:
  batch_size: 6                        # Reduced from 8 for memory
  num_workers: 4
  pin_memory: true

# Training
training:
  max_epochs: 50
  warmup_steps: 500
  learning_rate: 5.0e-5                # Conservative LR for stability
  min_lr: 1.0e-6
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  grad_clip: 1.0

  # Loss weights (V2: added gradient loss)
  lambda_pde: 1.0                      # PDE residual loss weight
  lambda_rmse: 1.0                     # RMSE loss weight
  lambda_grad: 0.1                     # V2: Gradient-aware loss weight

  # Use bf16 for transformer, but FFT needs float32
  # Training script handles dtype conversion
  mixed_precision: "bf16"
  gradient_accumulation_steps: 1
  eval_interval: 100                   # Validate every N steps
  early_stopping_patience: 30

# Logging
logging:
  project: "pde-sut-fno-v2"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints_sut_fno_burgers_v2"
  log_interval: 10
