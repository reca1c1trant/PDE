# Pretraining Configuration for PDE Foundation Model
# 18 channels: 3 vector + 15 scalar (expanded from 6)
# Datasets: diffusion-reaction, 2D_CFD, SWE, NS_incom

model_name: "pde_pretrain_scratch"

# Dataset (for pretrain, path points to root containing pretrained/ folder)
dataset:
  path: "/home/msai/song0304/code/PDE/data"
  seed: 42
  train_ratio: 0.9
  clips_ratio: 0.25  # 25% temporal sampling for training

# DataLoader
dataloader:
  batch_size: 8
  num_workers: 4
  pin_memory: true

# Model (18 channels)
model:
  in_channels: 18        # 3 vector + 15 scalar
  vector_channels: 3
  scalar_channels: 15
  noise_level: 0.05
  use_flash_attention: false
  gradient_checkpointing: true

  # V2 Encoder with ResBlocks
  encoder:
    version: "v2"
    channels: [64, 128, 256]
    use_resblock: true

  # Transformer (same as e2e_medium)
  transformer:
    hidden_size: 768
    num_hidden_layers: 10
    num_attention_heads: 12
    num_key_value_heads: 4
    intermediate_size: 3072
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

# Training
training:
  max_epochs: 25
  warmup_steps: 500
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  grad_clip: 5.0
  min_lr: 1.0e-6

  mixed_precision: "no"  # Use bf16 for pretraining
  gradient_accumulation_steps: 1

  eval_interval: 2
  early_stopping_patience: 50

# Checkpoint (optional: load from converted checkpoint)
checkpoint:
  resume_from: null  # Set to path like "checkpoints/converted.pt" to resume
  load_transformer_only: false  # If true, only load transformer weights

# Logging
logging:
  project: "pde-pretrain"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints_pretrain_scratch"
  log_interval: 10
