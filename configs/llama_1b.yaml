# PDE Causal AR Training Configuration
# Model: Llama 1B (DDP)

model_name: "llama_1b"

# Dataset
dataset:
  path: "/home/msai/song0304/code/PDE/data"
  temporal_length: 16
  train_ratio: 0.9
  seed: 42

# DataLoader
dataloader:
  batch_size: 2  # Per GPU (larger batch for smaller model)
  num_workers: 4
  pin_memory: true
  same_sample_per_batch: true  # true: 同一样本不同start_t, false: 不同样本

# Model
model:
  in_channels: 6
  noise_level: 0.05

  # Acceleration
  use_flash_attention: false  # Custom 4D mask not supported
  gradient_checkpointing: true

  # Llama 3.2 1B Transformer
  transformer:
    hidden_size: 2048
    num_hidden_layers: 16
    num_attention_heads: 32
    num_key_value_heads: 8  # GQA
    intermediate_size: 8192
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

# Training
training:
  max_steps: 2000
  resume_from: null           # Full resume (model + optimizer + scheduler)
  pretrain_from: checkpoints_1b/best4000.pt         # Load model weights only, restart optimizer/scheduler
  loss_alpha: 1             # 0=pure MSE, 1=pure RMSE/nRMSE, 0.5=balanced
  nrmse_sigma: [0.143810, 0.111739, 0.000000, 0.000000, 0.000000, 0.000000]           # null=use RMSE, [sigma_vx, sigma_vy, ...]=use nRMSE
  # Example: nrmse_sigma: [1.5, 1.2, 0.0, 0.8, 2.0, 1.0]  # Run compute_sigma.py to get values
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  grad_clip: 1.0
  min_lr: 1.0e-6
  warmup_steps: 20

  # Distributed: DDP for 1B model
  use_fsdp: false

  # Acceleration
  mixed_precision: "bf16"
  gradient_accumulation_steps: 1

  # Checkpointing
  save_every_steps: 1000
  eval_every_steps: 50
  keep_last_n_checkpoints: 2

  # Early stopping (null to disable)
  early_stopping_patience: 20

# Logging
logging:
  project: "pde-solver"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints_1b"
  log_interval: 10
