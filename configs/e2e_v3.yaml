# FFT Encoder V3 Training Configuration
# Uses train_fft.py with FFT-based encoder on 128x128 resolution

model_name: "pde_fft_v3"

# Dataset
dataset:
  path: "/home/msai/song0304/code/PDE/data"
  temporal_length: 16
  train_ratio: 0.9
  seed: 42

# DataLoader
dataloader:
  batch_size: 6
  num_workers: 4
  pin_memory: true
  same_sample_per_batch: false

# Model
model:
  in_channels: 6
  noise_level: 0.05
  use_flash_attention: false
  gradient_checkpointing: true  # Disabled for DDP compatibility

  # V3 FFT Encoder
  encoder:
    version: "v3"
    hidden_channels: 32   # FFT processing channels
    modes: 64             # Fourier modes (64 for 128x128 input)
    n_blocks: 4           # Number of FFT blocks

  # Transformer (same as V2)
  transformer:
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    num_key_value_heads: 4
    intermediate_size: 3072
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

# Training
training:
  max_steps: 4000
  warmup_steps: 200
  learning_rate: 1.0e-5
  weight_decay: 1.0e-5
  betas: [0.9, 0.999]
  grad_clip: 5.0
  min_lr: 1.0e-7

  # nRMSE sigma (used after warmup)
  nrmse_sigma: [0.143810, 0.111739, 0.0, 0.0, 0.0, 0.0]

  mixed_precision: "bf16"
  gradient_accumulation_steps: 1

  #save_every_steps: 500
  eval_every_steps: 20
  early_stopping_patience: 20

# Logging
logging:
  project: "pde-e2e"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints_fft_v3"
  log_interval: 10
