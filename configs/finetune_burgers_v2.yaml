# Burgers2D LoRA V2 Finetuning Configuration
# Key difference: Encoder and Decoder are UNFROZEN (trainable)
#
# This allows spatial feature adaptation to new physics (Burgers equation)
# while still using LoRA for efficient Transformer fine-tuning.

# Model
model:
  pretrained_path: "./checkpoints_e2e_medium_full_v3/best.pt"

  # Must match pretrained model config
  in_channels: 6
  noise_level: 0.0
  use_flash_attention: false
  gradient_checkpointing: true

  # Encoder config (must match pretrained)
  encoder:
    version: "v2"
    channels: [64, 128, 256]
    use_resblock: true

  # Transformer config (must match pretrained)
  transformer:
    hidden_size: 768
    num_hidden_layers: 10
    num_attention_heads: 12
    num_key_value_heads: 4
    intermediate_size: 3072
    hidden_act: "silu"
    max_position_embeddings: 4096
    rms_norm_eps: 1.0e-5
    rope_theta: 500000.0
    attention_dropout: 0.0

  # LoRA config (applied to Transformer only)
  lora:
    r: 16
    alpha: 32
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

# Dataset
dataset:
  path: "/scratch-share/SONG0304/finetune/burgers2d_nu0.1_0.15_n500.h5"
  temporal_length: 16
  train_ratio: 0.9
  seed: 42
  clips_per_sample: null

# Physics parameters
physics:
  dt: 0.001001001  # 1/999
  dx: 0.007874016  # 1/127
  dy: 0.007874016  # 1/127

# DataLoader
dataloader:
  batch_size: 8
  num_workers: 4
  pin_memory: true

# Training
training:
  max_epochs: 15
  warmup_steps: 200
  learning_rate: 1.0e-4
  min_lr: 1.0e-6
  weight_decay: 1.0e-4
  betas: [0.9, 0.999]
  grad_clip: 5.0

  # Loss weights (same as UNet baseline)
  lambda_pde: 0.01
  lambda_bc: 1.0

  mixed_precision: "no"
  gradient_accumulation_steps: 1
  eval_interval: 100
  early_stopping_patience: 20

# Logging
logging:
  project: "pde-burgers-lora"
  entity: "reca1c1trant-ntu"
  save_dir: "./checkpoints_burgers_lora_v2"
  log_interval: 10
